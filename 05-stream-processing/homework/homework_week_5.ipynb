{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/02/29 17:16:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.master('local[*]').appName('test').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.3.2'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Q1:\n",
    "spark.version\n",
    "#3.3.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 2:\n",
    "FHV October 2019\n",
    "\n",
    "Read the October 2019 FHV into a Spark Dataframe with a schema as we did in the lessons.\n",
    "\n",
    "Repartition the Dataframe to 6 partitions and save it to parquet.\n",
    "\n",
    "What is the average size of the Parquet (ending with .parquet extension) Files that were created (in MB)? Select the answer which most closely matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget https://github.com/DataTalksClub/nyc-tlc-data/releases/download/fhv/fhv_tripdata_2019-10.csv.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "fhv = spark.read.csv('./data/fhv_tripdata_2019-10.csv.gz', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "fhv.repartition(6).write.parquet('./data/parquet/fhv', mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39M\t./data/parquet/fhv\n",
      "6.4M\t./data/parquet/fhv/part-00005-ed315cf3-c371-4733-b471-de102b04254b-c000.snappy.parquet\n",
      "6.4M\t./data/parquet/fhv/part-00004-ed315cf3-c371-4733-b471-de102b04254b-c000.snappy.parquet\n",
      "6.4M\t./data/parquet/fhv/part-00003-ed315cf3-c371-4733-b471-de102b04254b-c000.snappy.parquet\n",
      "6.4M\t./data/parquet/fhv/part-00002-ed315cf3-c371-4733-b471-de102b04254b-c000.snappy.parquet\n",
      "6.4M\t./data/parquet/fhv/part-00001-ed315cf3-c371-4733-b471-de102b04254b-c000.snappy.parquet\n",
      "6.4M\t./data/parquet/fhv/part-00000-ed315cf3-c371-4733-b471-de102b04254b-c000.snappy.parquet\n",
      "52K\t./data/parquet/fhv/.part-00005-ed315cf3-c371-4733-b471-de102b04254b-c000.snappy.parquet.crc\n",
      "52K\t./data/parquet/fhv/.part-00004-ed315cf3-c371-4733-b471-de102b04254b-c000.snappy.parquet.crc\n",
      "52K\t./data/parquet/fhv/.part-00003-ed315cf3-c371-4733-b471-de102b04254b-c000.snappy.parquet.crc\n",
      "52K\t./data/parquet/fhv/.part-00002-ed315cf3-c371-4733-b471-de102b04254b-c000.snappy.parquet.crc\n",
      "52K\t./data/parquet/fhv/.part-00001-ed315cf3-c371-4733-b471-de102b04254b-c000.snappy.parquet.crc\n",
      "52K\t./data/parquet/fhv/.part-00000-ed315cf3-c371-4733-b471-de102b04254b-c000.snappy.parquet.crc\n",
      "4.0K\t./data/parquet/fhv/._SUCCESS.crc\n",
      "0\t./data/parquet/fhv/_SUCCESS\n"
     ]
    }
   ],
   "source": [
    "!du -ah ./data/parquet/fhv | sort -rh\n",
    "#6.4M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 3:\n",
    "Count records\n",
    "\n",
    "How many taxi trips were there on the 15th of October?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "|dispatching_base_num|    pickup_datetime|   dropOff_datetime|PUlocationID|DOlocationID|SR_Flag|Affiliated_base_number|\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "|              B00009|2019-10-01 00:23:00|2019-10-01 00:35:00|         264|         264|   null|                B00009|\n",
      "|              B00013|2019-10-01 00:11:29|2019-10-01 00:13:22|         264|         264|   null|                B00013|\n",
      "|              B00014|2019-10-01 00:11:43|2019-10-01 00:37:20|         264|         264|   null|                B00014|\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fhv.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "62610"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fhv.filter((fhv.pickup_datetime >= '2019-10-15 00:00:00') & (fhv.pickup_datetime < '2019-10-16 00:00:00')).count()\n",
    "#62610 trips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, unix_timestamp, max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 4:\n",
    "Longest trip for each day\n",
    "\n",
    "What is the length of the longest trip in the dataset in hours?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "fhv_duration = fhv.withColumn('pickup_seconds', unix_timestamp(col('pickup_datetime')))\n",
    "fhv_duration = fhv_duration.withColumn('dropoff_seconds', unix_timestamp(col('dropOff_datetime')))\n",
    "fhv_duration = fhv_duration.withColumn('duration', (fhv_duration.dropoff_seconds - fhv_duration.pickup_seconds)/3600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 20:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|max(duration)|\n",
      "+-------------+\n",
      "|     631152.5|\n",
      "+-------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "fhv_duration.select(max('duration')).show()\n",
    "#631 152.5 hours (weird, but ok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I wanted to practice querying in spark sql, so:\n",
    "fhv_duration.createTempView('duration_table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 24:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|duration|\n",
      "+--------+\n",
      "|631152.5|\n",
      "+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "SELECT duration\n",
    "FROM duration_table\n",
    "ORDER BY duration DESC\n",
    "LIMIT 1\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 5:\n",
    "User Interface\n",
    "\n",
    "Sparkâ€™s User Interface which shows the application's dashboard runs on which local port?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PORT 4040"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 6:\n",
    "Least frequent pickup location zone\n",
    "\n",
    "Load the zone lookup data into a temp view in Spark\n",
    "\n",
    "Using the zone lookup data and the FHV October 2019 data, what is the name of the LEAST frequent pickup location Zone?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget -P ./data https://github.com/DataTalksClub/nyc-tlc-data/releases/download/misc/taxi_zone_lookup.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "zones = spark.read.csv('./data/taxi_zone_lookup.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "zones.createTempView('zones_table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 38:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+----------------------+\n",
      "|LocationID|       Zone|count(pickup_datetime)|\n",
      "+----------+-----------+----------------------+\n",
      "|         2|Jamaica Bay|                     1|\n",
      "+----------+-----------+----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT zt.LocationID, zt.Zone, COUNT(dt.pickup_datetime)\n",
    "FROM duration_table dt JOIN zones_table zt on dt.PUlocationID = zt.LocationID\n",
    "GROUP BY zt.LocationID, zt.Zone\n",
    "ORDER BY COUNT(dt.pickup_datetime) ASC\n",
    "LIMIT 1;\n",
    "\"\"\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
